================================================================================
HYBRID2 BASELINE MODEL - FORWARD PASS TRACING
================================================================================
Model Configuration:
- Model Type: Hybrid2Baseline
- Input Size: 224×224
- EfficientNet Variant: b4
- Decoder Channels: [256, 128, 64, 32]
- Encoder Channels: [96, 192, 384, 768]
- Normalization: GroupNorm (default: enabled)
- Bottleneck: 2 Swin Transformer Blocks (enabled)
- Positional Embeddings: Enabled (default)
- Deep Supervision: Disabled (baseline)
- CBAM: Disabled (baseline)
- Smart Skip: Disabled (baseline)
- Cross-Attention: Disabled (baseline)
- Multi-Scale Aggregation: Disabled (baseline)

Example Input: Batch of 2 RGB images
Input Shape: (B=2, C=3, H=224, W=224)

================================================================================
STEP 1: ENCODER (Swin Transformer)
================================================================================

1.1 Patch Embedding
    Input:  (2, 3, 224, 224)
    Operation: Conv2d(3 → 96, kernel=4×4, stride=4)
    Output: (2, 96, 56, 56)  [H/4 × W/4]
    Format: Converted to tokens: (2, 3136, 96)  [56×56=3136 patches]
    Normalization: LayerNorm (Transformer standard)

1.2 Encoder Stage 1
    Input:  (2, 3136, 96) tokens
    Processing: 2 Swin Transformer Blocks
                - Window size: 7×7
                - Num heads: 3
                - Shifted window attention
    Output: (2, 3136, 96) tokens
    Convert to feature map: (2, 96, 56, 56)
    Normalization: LayerNorm (in Swin blocks)
    Feature: f1 = (2, 96, 56, 56)  ✓ Saved for skip connection

1.3 Encoder Stage 2
    Input:  (2, 3136, 96) tokens
    Processing: Patch Merging (2×2 downsampling)
                - Concatenate 4 patches → (2, 784, 192)
                - 2 Swin Transformer Blocks
                - Window size: 7×7
                - Num heads: 6
    Output: (2, 784, 192) tokens
    Convert to feature map: (2, 192, 28, 28)
    Normalization: LayerNorm (in Swin blocks)
    Feature: f2 = (2, 192, 28, 28)  ✓ Saved for skip connection

1.4 Encoder Stage 3
    Input:  (2, 784, 192) tokens
    Processing: Patch Merging (2×2 downsampling)
                - Concatenate 4 patches → (2, 196, 384)
                - 2 Swin Transformer Blocks
                - Window size: 7×7
                - Num heads: 12
    Output: (2, 196, 384) tokens
    Convert to feature map: (2, 384, 14, 14)
    Normalization: LayerNorm (in Swin blocks)
    Feature: f3 = (2, 384, 14, 14)  ✓ Saved for skip connection

1.5 Encoder Stage 4
    Input:  (2, 196, 384) tokens
    Processing: Patch Merging (2×2 downsampling)
                - Concatenate 4 patches → (2, 49, 768)
                - 2 Swin Transformer Blocks
                - Window size: 7×7
                - Num heads: 24
    Output: (2, 49, 768) tokens
    Convert to feature map: (2, 768, 7, 7)
    Normalization: LayerNorm (in Swin blocks)
    Feature: f4 = (2, 768, 7, 7)  ✓ Saved for skip connection

Encoder Output: [f1, f2, f3, f4]
    f1: (2, 96, 56, 56)   - Stage 1, resolution H/4 × W/4
    f2: (2, 192, 28, 28)  - Stage 2, resolution H/8 × W/8
    f3: (2, 384, 14, 14)  - Stage 3, resolution H/16 × W/16
    f4: (2, 768, 7, 7)    - Stage 4, resolution H/32 × W/32

================================================================================
STEP 2: ENCODER PROJECTIONS (Channel Alignment for Decoder)
================================================================================

Project encoder features to decoder channel dimensions:

2.1 Project f1 (Stage 1)
    Input:  f1 = (2, 96, 56, 56)
    Operation: Conv2d(96 → 256, kernel=1×1)
              → GroupNorm(256, num_groups=32)  ✓ GroupNorm
              → ReLU
    Output: p1 = (2, 256, 56, 56)

2.2 Project f2 (Stage 2)
    Input:  f2 = (2, 192, 28, 28)
    Operation: Conv2d(192 → 128, kernel=1×1)
              → GroupNorm(128, num_groups=32)  ✓ GroupNorm
              → ReLU
    Output: p2 = (2, 128, 28, 28)

2.3 Project f3 (Stage 3)
    Input:  f3 = (2, 384, 14, 14)
    Operation: Conv2d(384 → 64, kernel=1×1)
              → GroupNorm(64, num_groups=32)  ✓ GroupNorm
              → ReLU
    Output: p3 = (2, 64, 14, 14)

2.4 Project f4 (Stage 4)
    Input:  f4 = (2, 768, 7, 7)
    Operation: Conv2d(768 → 32, kernel=1×1)
              → GroupNorm(32, num_groups=32)  ✓ GroupNorm
              → ReLU
    Output: p4 = (2, 32, 7, 7)

Projected Features:
    p1: (2, 256, 56, 56)   - For skip connection at decoder stage 3
    p2: (2, 128, 28, 28)   - For skip connection at decoder stage 2
    p3: (2, 64, 14, 14)    - For skip connection at decoder stage 1
    p4: (2, 32, 7, 7)      - Bottleneck input

================================================================================
STEP 3: BOTTLENECK PROCESSING
================================================================================

3.1 Multi-Scale Aggregation (DISABLED in baseline)
    Status: Skipped (use_multiscale_agg=False)

3.2 Get Stage 4 Tokens from Encoder
    Source: Encoder Stage 4 output (before conversion to feature maps)
    Format: Token format from Swin Transformer encoder
    Shape: (2, 49, 768)  [B, L, C]
           - B = 2 (batch size)
           - L = 49 (7×7 = 49 tokens)
           - C = 768 (encoder Stage 4 dimension)
    Note: These are the original tokens from Stage 4, not feature maps

3.3 Project Token Dimension (768 → 32)
    Input:  Stage 4 tokens = (2, 49, 768)
    Operation: Linear projection (768 → 32)
              - Project encoder dimension to decoder bottleneck dimension
              - This aligns with decoder_channels[3] = 32
    Output: (2, 49, 32) tokens
    Note: Dimension reduction from encoder (768) to decoder (32)

3.4 Swin Transformer Bottleneck Blocks
    Input:  x_tokens = (2, 49, 32)  # Tokens from Stage 4 (projected)
    Processing: 2 Swin Transformer Blocks (BasicLayer)
                - Dimension: 32
                - Resolution: 7×7 (49 tokens)
                - Num heads: 8 (adaptive: 32/8=4 per head)
                - Window size: 7×7
                - Depth: 2 blocks
                - Drop path rate: 0.1
                - Normalization: LayerNorm (in Swin blocks)
    
    Block 1:
        - Window Attention (7×7 windows)
        - Shifted window attention
        - MLP (hidden_dim = 32 × 4 = 128)
        - LayerNorm
    Block 2:
        - Window Attention (7×7 windows)
        - Shifted window attention
        - MLP (hidden_dim = 32 × 4 = 128)
        - LayerNorm
    
    Output: x_tokens = (2, 49, 32) tokens

3.5 Convert Tokens to Feature Maps (for CNN Decoder)
    Input:  x_tokens = (2, 49, 32)  [Token format: B, L, C]
    Operation: Same technique as encoder output conversion
              - B, L, C = (2, 49, 32)
              - H = W = sqrt(L) = sqrt(49) = 7
              - Reshape: transpose(1, 2) → (2, 32, 49)
              - View: view(B, C, H, W) → (2, 32, 7, 7)
    Output: x = (2, 32, 7, 7)  [Feature map format: B, C, H, W]
    Note: Conversion from token format to CNN feature map format

3.6 CBAM Attention (DISABLED in baseline)
    Status: Skipped (use_cbam=False)

3.7 Positional Embeddings (ENABLED)
    Input:  x = (2, 32, 7, 7)
    Operation: PositionalEmbedding2D
              - Learnable 2D positional embeddings
              - Shape: (1, 32, 7, 7)
              - Added element-wise: x + pos_embed
    Output: x = (2, 32, 7, 7)

3.8 Cross-Attention (DISABLED in baseline)
    Status: Skipped (use_cross_attn=False)

Bottleneck Output: x = (2, 32, 7, 7)  [Feature map format ready for CNN decoder]

================================================================================
STEP 4: DECODER STAGE 1 (H/32 → H/16)
================================================================================

4.1 Decoder Block 1
    Input:  x = (2, 32, 7, 7)
    Operation: SimpleDecoderBlock(32 → 64)
              - Conv2d(32 → 64, kernel=3×3, padding=1)
                → GroupNorm(64, num_groups=32)  ✓ GroupNorm
                → ReLU
              - Conv2d(64 → 64, kernel=3×3, padding=1)
                → GroupNorm(64, num_groups=32)  ✓ GroupNorm
                → ReLU
    Output: (2, 64, 7, 7)

4.2 Upsampling
    Input:  (2, 64, 7, 7)
    Operation: Upsample(scale_factor=2, mode='bilinear')
    Output: (2, 64, 14, 14)

4.3 Skip Connection 1
    Decoder feature: (2, 64, 14, 14)
    Encoder feature: p3 = (2, 64, 14, 14)
    
    Operation: SimpleSkipConnection
              - Project encoder: Conv2d(64 → 64, kernel=1×1)
                                → GroupNorm(64)  ✓ GroupNorm
                                → ReLU
              - Concatenate: cat([decoder_feat, encoder_proj], dim=1)
                            → (2, 128, 14, 14)
              - Fuse: Conv2d(128 → 64, kernel=3×3)
                     → GroupNorm(64)  ✓ GroupNorm
                     → ReLU
    Output: x = (2, 64, 14, 14)

4.4 Deep Supervision (DISABLED)
    Status: Skipped (use_deep_supervision=False)

Decoder Stage 1 Output: x = (2, 64, 14, 14)

================================================================================
STEP 5: DECODER STAGE 2 (H/16 → H/8)
================================================================================

5.1 Decoder Block 2
    Input:  x = (2, 64, 14, 14)
    Operation: SimpleDecoderBlock(64 → 128)
              - Conv2d(64 → 128, kernel=3×3, padding=1)
                → GroupNorm(128, num_groups=32)  ✓ GroupNorm
                → ReLU
              - Conv2d(128 → 128, kernel=3×3, padding=1)
                → GroupNorm(128, num_groups=32)  ✓ GroupNorm
                → ReLU
    Output: (2, 128, 14, 14)

5.2 Upsampling
    Input:  (2, 128, 14, 14)
    Operation: Upsample(scale_factor=2, mode='bilinear')
    Output: (2, 128, 28, 28)

5.3 Skip Connection 2
    Decoder feature: (2, 128, 28, 28)
    Encoder feature: p2 = (2, 128, 28, 28)
    
    Operation: SimpleSkipConnection
              - Project encoder: Conv2d(128 → 128, kernel=1×1)
                                → GroupNorm(128)  ✓ GroupNorm
                                → ReLU
              - Concatenate: cat([decoder_feat, encoder_proj], dim=1)
                            → (2, 256, 28, 28)
              - Fuse: Conv2d(256 → 128, kernel=3×3)
                     → GroupNorm(128)  ✓ GroupNorm
                     → ReLU
    Output: x = (2, 128, 28, 28)

5.4 Deep Supervision (DISABLED)
    Status: Skipped

Decoder Stage 2 Output: x = (2, 128, 28, 28)

================================================================================
STEP 6: DECODER STAGE 3 (H/8 → H/4)
================================================================================

6.1 Decoder Block 3
    Input:  x = (2, 128, 28, 28)
    Operation: SimpleDecoderBlock(128 → 256)
              - Conv2d(128 → 256, kernel=3×3, padding=1)
                → GroupNorm(256, num_groups=32)  ✓ GroupNorm
                → ReLU
              - Conv2d(256 → 256, kernel=3×3, padding=1)
                → GroupNorm(256, num_groups=32)  ✓ GroupNorm
                → ReLU
    Output: (2, 256, 28, 28)

6.2 Upsampling
    Input:  (2, 256, 28, 28)
    Operation: Upsample(scale_factor=2, mode='bilinear')
    Output: (2, 256, 56, 56)

6.3 Skip Connection 3
    Decoder feature: (2, 256, 56, 56)
    Encoder feature: p1 = (2, 256, 56, 56)
    
    Operation: SimpleSkipConnection
              - Project encoder: Conv2d(256 → 256, kernel=1×1)
                                → GroupNorm(256)  ✓ GroupNorm
                                → ReLU
              - Concatenate: cat([decoder_feat, encoder_proj], dim=1)
                            → (2, 512, 56, 56)
              - Fuse: Conv2d(512 → 256, kernel=3×3)
                     → GroupNorm(256)  ✓ GroupNorm
                     → ReLU
    Output: x = (2, 256, 56, 56)

6.4 Deep Supervision (DISABLED)
    Status: Skipped

Decoder Stage 3 Output: x = (2, 256, 56, 56)

================================================================================
STEP 7: DECODER STAGE 4 (H/4 → H)
================================================================================

7.1 Final Decoder Block
    Input:  x = (2, 256, 56, 56)
    Operation: Conv2d(256 → 64, kernel=3×3, padding=1)
              → GroupNorm(64, num_groups=32)  ✓ GroupNorm
              → ReLU
    Output: (2, 64, 56, 56)

7.2 Final Upsampling
    Input:  (2, 64, 56, 56)
    Operation: Upsample(scale_factor=4, mode='bilinear')
    Output: (2, 64, 224, 224)

Decoder Stage 4 Output: x = (2, 64, 224, 224)

================================================================================
STEP 8: SEGMENTATION HEAD
================================================================================

8.1 Segmentation Head
    Input:  x = (2, 64, 224, 224)
    Operation:
              - Conv2d(64 → 64, kernel=3×3, padding=1)
                → GroupNorm(64, num_groups=32)  ✓ GroupNorm
                → ReLU
              - Dropout2d(0.1)
              - Conv2d(64 → 6, kernel=1×1)  [6 classes]
    Output: (2, 6, 224, 224)

================================================================================
FINAL OUTPUT
================================================================================

Output Shape: (2, 6, 224, 224)
- Batch size: 2
- Classes: 6 (Background, Text, Comment, Decoration, Chapter Heading, Catchword)
- Resolution: 224×224 (same as input)

Output Format: Logits (before softmax)
- Each pixel has 6 class scores
- Apply softmax to get probabilities

================================================================================
NORMALIZATION SUMMARY
================================================================================

✓ GroupNorm Used In:
  1. Encoder Projections (all 4 stages)
  2. Decoder Blocks (all 4 stages, both conv layers)
  3. Skip Connections (projection and fusion layers)
  4. Segmentation Head

✓ LayerNorm Used In:
  1. Encoder (Swin Transformer blocks)
  2. Bottleneck (Swin Transformer blocks)

✓ Positional Embeddings:
  1. Bottleneck (2D learnable positional embeddings)

================================================================================
FEATURE RESOLUTION PROGRESSION
================================================================================

Input:     224×224  (H × W)
Stage 1:   56×56    (H/4 × W/4)
Stage 2:   28×28    (H/8 × W/8)
Stage 3:   14×14    (H/16 × W/16)
Stage 4:   7×7      (H/32 × W/32)  [Bottleneck]
Decoder 1: 14×14    (H/16 × W/16)
Decoder 2: 28×28    (H/8 × W/8)
Decoder 3: 56×56    (H/4 × W/4)
Decoder 4: 224×224  (H × W)
Output:    224×224  (H × W)

================================================================================
CHANNEL PROGRESSION
================================================================================

Encoder Channels:  [96, 192, 384, 768]
Decoder Channels: [256, 128, 64, 32]
Projected Channels: [256, 128, 64, 32]  (aligned with decoder)
Final Output: 6 classes

================================================================================
END OF TRACING
================================================================================
