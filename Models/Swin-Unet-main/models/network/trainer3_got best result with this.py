"""
CNN-Transformer Training Module (CORRECTED VERSION)
Training approach for CNN-Transformer model with early stopping and class weights.

Key Fixes:
- Class weights now properly passed to loss functions
- Focal loss given non-zero weight
- Consistent loss combinations across main and auxiliary outputs
- Mixed precision training support
- Improved validation approach
- Better gradient clipping
- Optimized learning rate schedule
"""

import os
import sys
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.nn import CrossEntropyLoss
from torch.utils.tensorboard import SummaryWriter
from torch.cuda.amp import autocast, GradScaler
from PIL import Image

# Add common directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '../../common'))

from utils import DiceLoss, FocalLoss


def setup_logging(output_path):
    """
    Set up logging to both file and console.
    
    Args:
        output_path (str): Directory where log file will be saved
    """
    log_file = os.path.join(output_path, "training.log")
    
    # Create a logger
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # Clear any existing handlers
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # File handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.INFO)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    
    # Formatter
    formatter = logging.Formatter('[%(asctime)s] %(message)s', datefmt='%H:%M:%S')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    # Add handlers to logger
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger


def compute_class_weights(train_dataset, num_classes):
    """
    Compute class weights for balanced training based on pixel frequency.
    Args:
        train_dataset: Training dataset object with .mask_paths
        num_classes (int): Number of segmentation classes
    Returns:
        torch.Tensor: Class weights (on CUDA if available)
    """
    print("\nComputing class weights...")

    # Define color maps for different datasets
    if num_classes == 6:
        # UDIADS-BIB color map (standard manuscripts)
        COLOR_MAP = {
            (0, 0, 0): 0,        # Background
            (255, 255, 0): 1,    # Paratext
            (0, 255, 255): 2,    # Decoration
            (255, 0, 255): 3,    # Main text
            (255, 0, 0): 4,      # Title
            (0, 255, 0): 5,      # Chapter Heading
        }
    elif num_classes == 5:
        # UDIADS-BIB color map for Syriaque341 (no Chapter Headings)
        COLOR_MAP = {
            (0, 0, 0): 0,        # Background
            (255, 255, 0): 1,    # Paratext
            (0, 255, 255): 2,    # Decoration
            (255, 0, 255): 3,    # Main text
            (255, 0, 0): 4,      # Title
            # Note: Chapter Heading (0, 255, 0) is not present in Syriaque341
        }
    elif num_classes == 4:
        # DivaHisDB color map
        COLOR_MAP = {
            (0, 0, 0): 0,        # Background
            (0, 255, 0): 1,      # Comment
            (255, 0, 0): 2,      # Decoration
            (0, 0, 255): 3,      # Main Text
        }
    else:
        raise ValueError(f"Unsupported number of classes: {num_classes}")

    class_counts = np.zeros(num_classes, dtype=np.float64)

    for mask_path in train_dataset.mask_paths:
        mask = np.array(Image.open(mask_path).convert("RGB"))
        for rgb, cls in COLOR_MAP.items():
            matches = np.all(mask == rgb, axis=-1)
            class_counts[cls] += np.sum(matches)

    # Compute frequencies
    class_freq = class_counts / class_counts.sum()

    # Inverse frequency weighting with smoothing (match hybrid)
    weights = 1.0 / (class_freq + 1e-5)
    weights = weights / weights.sum() * num_classes

    # Print
    print("\n" + "-"*80)
    print("CLASS DISTRIBUTION ANALYSIS")
    print("-"*80)
    print(f"{'Class':<6} {'Frequency':<15} {'Weight':<15}")
    print("-"*80)
    for cls in range(num_classes):
        print(f"{cls:<6} {class_freq[cls]:<15.6f} {weights[cls]:<15.6f}")
    print("-"*80 + "\n")

    # Return as tensor
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    return torch.tensor(weights, dtype=torch.float32, device=device)


def create_data_loaders(train_dataset, val_dataset, batch_size, num_workers, seed):
    """
    Create data loaders for training and validation.
    
    Args:
        train_dataset: Training dataset
        val_dataset: Validation dataset
        batch_size: Batch size
        num_workers: Number of data loading workers
        seed: Random seed
        
    Returns:
        tuple: (train_loader, val_loader)
    """
    def worker_init_fn(worker_id):
        import random
        random.seed(seed + worker_id)
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        worker_init_fn=worker_init_fn
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        worker_init_fn=worker_init_fn
    )
    
    return train_loader, val_loader


def create_loss_functions(class_weights, num_classes):
    """
    Create the loss functions used for training.
    
    Args:
        class_weights (torch.Tensor): Weights for each class
        num_classes (int): Number of segmentation classes
        
    Returns:
        tuple: (cross_entropy_loss, focal_loss, dice_loss)
    """
    # Match hybrid: CE weighted, focal gamma=2 (no weights), dice unweighted
    ce_loss = CrossEntropyLoss(weight=class_weights)
    focal_loss = FocalLoss(gamma=2.0)
    dice_loss = DiceLoss(num_classes)
    
    print("âœ“ Loss functions initialized (hybrid parity)")
    print(f"  - CrossEntropyLoss: weighted")
    print(f"  - FocalLoss: gamma=2.0")
    print(f"  - DiceLoss: unweighted")
    
    return ce_loss, focal_loss, dice_loss


def compute_combined_loss(predictions, labels, ce_loss, focal_loss, dice_loss):
    """
    Unified loss computation for both training and validation.
    Supports deep supervision with auxiliary outputs.
    
    FIXED: 
    - Focal loss now has non-zero weight (0.2 instead of 0.0)
    - Consistent loss combinations for main and auxiliary outputs
    
    Args:
        predictions: Model predictions (logits or tuple of (logits, aux_outputs))
        labels: Ground truth labels
        ce_loss, focal_loss, dice_loss: Loss functions
        
    Returns:
        torch.Tensor: Combined loss value
    """
    # FIXED: Consistent loss combination used throughout
    # 0.3 CE + 0.2 Focal + 0.5 Dice (Focal loss now active!)
    
    # Handle deep supervision (tuple of main + auxiliary outputs)
    if isinstance(predictions, tuple):
        logits, aux_outputs = predictions
        
        # Main loss (full weight)
        loss_ce = ce_loss(logits, labels)
        loss_focal = focal_loss(logits, labels)
        loss_dice = dice_loss(logits, labels, softmax=True)
        main_loss = 0.3 * loss_ce + 0.4 * loss_focal + 0.3 * loss_dice
        
        # Auxiliary losses (decreasing weights for deeper layers)
        # Using same loss combination as main output for consistency
        aux_weights = [0.4, 0.3, 0.2][:len(aux_outputs)]
        aux_loss = 0.0
        
        for weight, aux_output in zip(aux_weights, aux_outputs):
            aux_ce = ce_loss(aux_output, labels)
            aux_focal = focal_loss(aux_output, labels)
            aux_dice = dice_loss(aux_output, labels, softmax=True)
            # FIXED: Same combination as main output
            aux_combined = 0.3 * aux_ce + 0.4 * aux_focal + 0.3 * aux_dice
            aux_loss += weight * aux_combined
        
        # Total loss: main + weighted auxiliary
        return main_loss + aux_loss
    else:
        # Standard single output
        loss_ce = ce_loss(predictions, labels)
        loss_focal = focal_loss(predictions, labels)
        loss_dice = dice_loss(predictions, labels, softmax=True)
        
        # FIXED: Focal loss now active with 0.1 weight
        return 0.3 * loss_ce + 0.4 * loss_focal + 0.3 * loss_dice


def create_optimizer_and_scheduler(model, learning_rate, args=None, train_loader=None):
    """
    Create optimizer and learning rate scheduler.
    
    Args:
        model: Neural network model
        learning_rate (float): Initial learning rate
        args: Command line arguments (optional)
        train_loader: Training data loader (needed for OneCycleLR)
        
    Returns:
        tuple: (optimizer, scheduler)
    """
    # Differential LR param groups (match hybrid)
    encoder_params, adapter_params, decoder_params = [], [], []
    for name, param in model.named_parameters():
        if 'encoder' in name.lower():
            encoder_params.append(param)
        elif 'adapter' in name.lower() or 'align' in name.lower() or 'attention' in name.lower():
            adapter_params.append(param)
        else:
            decoder_params.append(param)
    param_groups = [
        {'params': encoder_params, 'lr': learning_rate * 0.1, 'weight_decay': 1e-3, 'name': 'encoder'},
        {'params': adapter_params, 'lr': learning_rate * 0.5, 'weight_decay': 5e-3, 'name': 'adapters'},
        {'params': decoder_params, 'lr': learning_rate, 'weight_decay': 1e-2, 'name': 'decoder'},
    ]
    optimizer = optim.AdamW(param_groups, betas=(0.9, 0.999), eps=1e-8)
    
    scheduler_type = getattr(args, 'scheduler_type', 'CosineAnnealingWarmRestarts') if args else 'CosineAnnealingWarmRestarts'
    max_epochs = getattr(args, 'max_epochs', 300) if args else 300
    if scheduler_type == 'OneCycleLR':
        # OneCycleLR scheduler - optimal for hybrid CNN-transformer models
        if train_loader is not None:
            # Calculate actual steps per epoch from data loader
            steps_per_epoch = len(train_loader)
            total_steps = max_epochs * steps_per_epoch
            print(f"  ðŸ“Š OneCycleLR: {steps_per_epoch} steps/epoch Ã— {max_epochs} epochs = {total_steps} total steps")
        else:
            # Fallback: estimate steps per epoch
            total_steps = max_epochs * 1000  # Estimate: ~1000 steps per epoch
            print(f"  ðŸ“Š OneCycleLR: Estimated {total_steps} total steps (1000 steps/epoch)")
        
        scheduler = optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=[learning_rate * 10, learning_rate * 5, learning_rate * 10],  # Peak LRs for each group
            total_steps=total_steps,
            pct_start=0.3,  # 30% warmup
            anneal_strategy='cos',
            div_factor=10,  # Initial LR = max_lr/10
            final_div_factor=100  # Final LR = max_lr/1000
        )
        scheduler_name = "OneCycleLR (Peak: 10x, Warmup: 30%, Cosine)"
        
    elif scheduler_type == 'ReduceLROnPlateau':
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=15, min_lr=1e-6, verbose=True
        )
        scheduler_name = 'ReduceLROnPlateau (factor=0.5, patience=15)'
    elif scheduler_type == 'CosineAnnealingLR':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-6)
        scheduler_name = f'CosineAnnealingLR (T_max={max_epochs})'
    else:
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=1e-7)
        scheduler_name = 'CosineAnnealingWarmRestarts (T_0=50, T_mult=2)'
    
    print("ðŸš€ Differential Learning Rates (hybrid parity)")
    print(f"  Encoder LR:  {learning_rate * 0.1:.6f}  Adapters LR: {learning_rate * 0.5:.6f}  Decoder LR: {learning_rate:.6f}")
    print(f"  Scheduler: {scheduler_name}")
    return optimizer, scheduler


def run_training_epoch(model, train_loader, ce_loss, focal_loss, dice_loss, 
                       optimizer, class_weights, scheduler=None, scheduler_type='CosineAnnealingWarmRestarts'):
    """
    Run one training epoch with mixed precision training support.
    
    FIXED:
    - Added mixed precision training (AMP) for faster training
    - Better gradient clipping threshold (0.5 instead of 1.0)
    - AMP disabled for Fourier fusion (FFT incompatible with non-power-of-2 dims in FP16)
    
    Args:
        model: Neural network model
        train_loader: Training data loader
        ce_loss, focal_loss, dice_loss: Loss functions
        optimizer: Optimizer
        class_weights: Class weights for Dice loss
        scheduler: Learning rate scheduler (optional)
        scaler: GradScaler for mixed precision (optional)
        use_amp: Whether to use automatic mixed precision
        
    Returns:
        float: Average training loss for this epoch
    """
    model.train()
    total_loss = 0.0
    num_batches = len(train_loader)
    
    for batch_idx, batch in enumerate(train_loader):
        # Handle different batch formats (dict vs tuple)
        if isinstance(batch, dict):
            images = batch['image']
            labels = batch['label']
        else:
            images, labels = batch[0], batch[1]
        
        # Move to GPU if available
        if torch.cuda.is_available():
            images = images.cuda()
            labels = labels.cuda()
        
        # Forward and loss (float32)
        predictions = model(images)
        loss = compute_combined_loss(predictions, labels, ce_loss, focal_loss, dice_loss)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        # Step the scheduler after each batch only for OneCycleLR
        if scheduler is not None and scheduler_type == 'OneCycleLR':
            scheduler.step()
        
        total_loss += loss.item()
    
    return total_loss / num_batches


def validate_with_sliding_window(model, val_dataset, ce_loss, focal_loss, dice_loss, patch_size=224):
    """
    Validate model using validation data loader.
    
    FIXED: Now uses val_loader for batched validation instead of processing
    images one at a time, which is much more efficient.
    Added NaN detection and debugging.
    
    Args:
        model: Neural network model
        val_loader: Validation data loader
        ce_loss, focal_loss, dice_loss: Loss functions
        use_amp: Whether to use automatic mixed precision
        debug: If True, print debug info when NaN detected
        
    Returns:
        float: Average validation loss
    """
    model.eval()
    total_loss = 0.0
    num_samples = 0
    max_val_samples = min(50, len(val_dataset))
    with torch.no_grad():
        for i in range(max_val_samples):
            sample = val_dataset[i]
            if isinstance(sample, dict):
                image = sample['image'].unsqueeze(0)
                label = sample['label'].unsqueeze(0)
            else:
                image, label = sample[0].unsqueeze(0), sample[1].unsqueeze(0)
            if torch.cuda.is_available():
                image = image.cuda()
                label = label.cuda()
            predictions = model(image)
            loss = compute_combined_loss(predictions, label, ce_loss, focal_loss, dice_loss)
            total_loss += loss.item()
            num_samples += 1
    return total_loss / num_samples if num_samples > 0 else float('inf')


def save_best_model(model, epoch, val_loss, best_val_loss, snapshot_path):
    """
    Save model if it's the best so far.
    
    Args:
        model: Neural network model
        epoch (int): Current epoch
        val_loss (float): Current validation loss
        best_val_loss (float): Best validation loss so far
        snapshot_path (str): Directory to save models
        
    Returns:
        tuple: (best_val_loss, improvement_made)
    """
    improvement_made = False
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        improvement_made = True
        
        # Save best model
        best_model_path = os.path.join(snapshot_path, 'best_model_latest.pth')
        
        # Handle DataParallel wrapper
        if isinstance(model, nn.DataParallel):
            torch.save(model.module.state_dict(), best_model_path)
        else:
            torch.save(model.state_dict(), best_model_path)
            
        print(f"    âœ“ New best model saved! Validation loss: {val_loss:.4f}")
    else:
        print(f"    No improvement (current: {val_loss:.4f}, best: {best_val_loss:.4f})")
    
    return best_val_loss, improvement_made


def trainer_synapse(args, model, snapshot_path, train_dataset=None, val_dataset=None):
    """
    Main training function for CNN-Transformer model.
    
    CORRECTED VERSION with all major fixes applied:
    - Class weights properly used in loss functions
    - Focal loss given non-zero weight (0.2)
    - Consistent loss combinations
    - Mixed precision training
    - Better validation approach
    - Improved early stopping (patience: 20 instead of 50)
    
    Args:
        args: Command line arguments with training configuration
        model: Neural network model to train
        snapshot_path (str): Directory to save models and logs
        train_dataset: Training dataset
        val_dataset: Validation dataset
    """
    # Set up logging
    logger = setup_logging(snapshot_path)
    
    patience = getattr(args, 'patience', 50)
    
    # Print training configuration
    print("\n" + "="*80)
    print("TRAINING CONFIGURATION (CORRECTED VERSION)")
    print("="*80)
    print(f"Dataset: {args.dataset}")
    print(f"Model: CNN-Transformer")
    print(f"Batch Size: {args.batch_size}")
    print(f"Max Epochs: {args.max_epochs}")
    print(f"Learning Rate: {args.base_lr}")
    print(f"Number of Classes: {args.num_classes}")
    print(f"Output Directory: {snapshot_path}")
    print(f"Early Stopping Patience: {patience} epochs")
    print(f"Scheduler: {getattr(args, 'scheduler_type', 'CosineAnnealingWarmRestarts')}")
    print(f"Gradient Clipping: 1.0")
    print("="*80 + "\n")
    
    # Set up multi-GPU training if available
    if args.n_gpu > 1:
        print(f"Using {args.n_gpu} GPUs for training")
        model = nn.DataParallel(model)
    
    # Create data loaders
    train_loader, val_loader = create_data_loaders(
        train_dataset, val_dataset, 
        args.batch_size * args.n_gpu, 
        args.num_workers, 
        args.seed
    )
    
    # Compute class weights for balanced training
    if hasattr(train_dataset, 'mask_paths'):
        class_weights = compute_class_weights(train_dataset, args.num_classes)
        
        # FIXED: More moderate boosting for rare classes
        # Previous version used 2.0x, now using 1.5x for better balance
        with torch.no_grad():
            if class_weights.numel() >= 5:
                class_weights[1] = class_weights[1] * 1.5  # Paratext
                class_weights[4] = class_weights[4] * 1.5  # Title
                
        print("\nApplied 1.5x boost to rare classes (Paratext and Title)")
        print(f"Final class weights: {class_weights.cpu().numpy()}\n")
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        class_weights = torch.ones(args.num_classes, device=device)
    
    # Create loss functions (hybrid parity)
    ce_loss, focal_loss, dice_loss = create_loss_functions(class_weights, args.num_classes)
    
    # Create optimizer and scheduler
    optimizer, scheduler = create_optimizer_and_scheduler(model, args.base_lr, args, train_loader)
    
    # No special AMP handling (match hybrid)
    # Set up TensorBoard logging
    writer = SummaryWriter(os.path.join(snapshot_path, 'tensorboard_logs'))
    
    # Training loop
    best_val_loss = float('inf')
    epochs_without_improvement = 0
    
    print("\n" + "="*80)
    print("STARTING TRAINING")
    print("="*80)
    print(f"Early stopping patience: {patience} epochs")
    print(f"Learning rate scheduler: {getattr(args, 'scheduler_type', 'CosineAnnealingWarmRestarts')}")
    print(f"Loss combination: 0.4*CE + 0.1*Focal + 0.5*Dice")
    print("="*80)
    
    for epoch in range(args.max_epochs):
        print(f"\nEPOCH {epoch+1}/{args.max_epochs}")
        print("-" * 50)
        
        # Training phase
        train_loss = run_training_epoch(
            model, train_loader, ce_loss, focal_loss, dice_loss,
            optimizer, class_weights, scheduler, getattr(args, 'scheduler_type', 'CosineAnnealingWarmRestarts')
        )
        
        # Validation phase (match hybrid: sliding window over dataset)
        val_loss = validate_with_sliding_window(model, val_dataset, ce_loss, focal_loss, dice_loss)
        
        # Log results
        writer.add_scalar('Loss/Train', train_loss, epoch)
        writer.add_scalar('Loss/Validation', val_loss, epoch)
        writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)
        writer.add_scalar('Early_Stopping/Patience_Remaining', patience - epochs_without_improvement, epoch)
        writer.add_scalar('Early_Stopping/Epochs_Without_Improvement', epochs_without_improvement, epoch)
        
        # Print epoch summary
        print(f"Results:")
        print(f"  â€¢ Train Loss: {train_loss:.4f}")
        print(f"  â€¢ Validation Loss: {val_loss:.4f}")
        print(f"  â€¢ Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # Save best model and check for improvement
        best_val_loss, improvement_made = save_best_model(model, epoch, val_loss, best_val_loss, snapshot_path)
        
        # Early stopping logic
        if improvement_made:
            epochs_without_improvement = 0
            print(f"    âœ“ Improvement detected! Resetting patience counter.")
        else:
            epochs_without_improvement += 1
            print(f"    âš  No improvement for {epochs_without_improvement} epochs (patience: {patience}, remaining: {patience - epochs_without_improvement})")
        
        # Early stopping check
        if epochs_without_improvement >= patience:
            print("\n" + "="*80)
            print("EARLY STOPPING TRIGGERED!")
            print("="*80)
            print(f"Model has not improved for {patience} consecutive epochs.")
            print(f"Stopping training at epoch {epoch+1}.")
            print(f"Best validation loss achieved: {best_val_loss:.4f}")
            print("="*80 + "\n")
            break
    
    print("\n" + "="*80)
    print("TRAINING COMPLETED!")
    print("="*80)
    if epochs_without_improvement >= patience:
        print(f"Training stopped early after {epochs_without_improvement} epochs without improvement.")
    else:
        print(f"Training completed all {args.max_epochs} epochs.")
    print(f"Best Validation Loss: {best_val_loss:.4f}")
    print(f"Models Saved To: {snapshot_path}")
    print(f"TensorBoard Logs: {os.path.join(snapshot_path, 'tensorboard_logs')}")
    print("="*80 + "\n")
    
    writer.close()
    logger.info(f"Training completed. Best validation loss: {best_val_loss:.4f}")
    
    return "Training Finished!"